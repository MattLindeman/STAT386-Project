{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023 Data\n",
    "# Scraping data from Barttorvik\n",
    "url = 'https://barttorvik.com/trank.php?year=2023&sort=&top=0&conlimit=All#'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "\n",
    "# Process the table data\n",
    "if table:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    headers = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all(['th', 'td'])\n",
    "        row_data = []\n",
    "        for i, col in enumerate(columns):\n",
    "            row_data.append(col.get_text())\n",
    "            if len(headers) < len(columns):  # Handle missing headers\n",
    "                headers.append(f'Column {i + 1}')\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Convert data into a DataFrame\n",
    "    df_2023 = pd.DataFrame(data)\n",
    "    # Set headers if available\n",
    "    if headers:\n",
    "        df_2023.columns = headers\n",
    "        \n",
    "# Making the proper row the column names\n",
    "df_2023.columns = df_2023.iloc[1]\n",
    "df_2023 = df_2023.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Getting rid of rows that have the column names\n",
    "first_two_columns = df_2023.columns[:2]\n",
    "df_2023 = df_2023[~(df_2023[first_two_columns].astype(str) == df_2023.iloc[:, :2].columns).all(axis=1)]\n",
    "\n",
    "# Adding a season column\n",
    "df_2023['Season'] = '22-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022 Data\n",
    "# Scraping data from Barttorvik\n",
    "url = 'https://barttorvik.com/trank.php?year=2022&sort=&top=0&conlimit=All#'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "\n",
    "# Process the table data\n",
    "if table:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    headers = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all(['th', 'td'])\n",
    "        row_data = []\n",
    "        for i, col in enumerate(columns):\n",
    "            row_data.append(col.get_text())\n",
    "            if len(headers) < len(columns):  # Handle missing headers\n",
    "                headers.append(f'Column {i + 1}')\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Convert data into a DataFrame\n",
    "    df_2022 = pd.DataFrame(data)\n",
    "    # Set headers if available\n",
    "    if headers:\n",
    "        df_2022.columns = headers\n",
    "        \n",
    "# Making the proper row the column names\n",
    "df_2022.columns = df_2022.iloc[1]\n",
    "df_2022 = df_2022.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Getting rid of rows that have the column names\n",
    "first_two_columns = df_2022.columns[:2]\n",
    "df_2022 = df_2022[~(df_2022[first_two_columns].astype(str) == df_2022.iloc[:, :2].columns).all(axis=1)]\n",
    "\n",
    "# Adding a season column\n",
    "df_2022['Season'] = '21-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021 Data\n",
    "# Scraping data from Barttorvik\n",
    "url = 'https://barttorvik.com/trank.php?year=2021&sort=&top=0&conlimit=All#'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "\n",
    "# Process the table data\n",
    "if table:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    headers = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all(['th', 'td'])\n",
    "        row_data = []\n",
    "        for i, col in enumerate(columns):\n",
    "            row_data.append(col.get_text())\n",
    "            if len(headers) < len(columns):  # Handle missing headers\n",
    "                headers.append(f'Column {i + 1}')\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Convert data into a DataFrame\n",
    "    df_2021 = pd.DataFrame(data)\n",
    "    # Set headers if available\n",
    "    if headers:\n",
    "        df_2021.columns = headers\n",
    "        \n",
    "# Making the proper row the column names\n",
    "df_2021.columns = df_2021.iloc[1]\n",
    "df_2021 = df_2021.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Getting rid of rows that have the column names\n",
    "first_two_columns = df_2021.columns[:2]\n",
    "df_2021 = df_2021[~(df_2021[first_two_columns].astype(str) == df_2021.iloc[:, :2].columns).all(axis=1)]\n",
    "\n",
    "# Adding a season column\n",
    "df_2021['Season'] = '20-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020 Data\n",
    "# Scraping data from Barttorvik\n",
    "url = 'https://barttorvik.com/trank.php?year=2020&sort=&top=0&conlimit=All#'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "\n",
    "# Process the table data\n",
    "if table:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    headers = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all(['th', 'td'])\n",
    "        row_data = []\n",
    "        for i, col in enumerate(columns):\n",
    "            row_data.append(col.get_text())\n",
    "            if len(headers) < len(columns):  # Handle missing headers\n",
    "                headers.append(f'Column {i + 1}')\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Convert data into a DataFrame\n",
    "    df_2020 = pd.DataFrame(data)\n",
    "    # Set headers if available\n",
    "    if headers:\n",
    "        df_2020.columns = headers\n",
    "        \n",
    "# Making the proper row the column names\n",
    "df_2020.columns = df_2020.iloc[1]\n",
    "df_2020 = df_2020.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Getting rid of rows that have the column names\n",
    "first_two_columns = df_2020.columns[:2]\n",
    "df_2020 = df_2020[~(df_2020[first_two_columns].astype(str) == df_2020.iloc[:, :2].columns).all(axis=1)]\n",
    "\n",
    "# Adding a season column\n",
    "df_2020['Season'] = '19-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 Data\n",
    "# Scraping data from Barttorvik\n",
    "url = 'https://barttorvik.com/trank.php?year=2019&sort=&top=0&conlimit=All#'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "\n",
    "# Process the table data\n",
    "if table:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    headers = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all(['th', 'td'])\n",
    "        row_data = []\n",
    "        for i, col in enumerate(columns):\n",
    "            row_data.append(col.get_text())\n",
    "            if len(headers) < len(columns):  # Handle missing headers\n",
    "                headers.append(f'Column {i + 1}')\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Convert data into a DataFrame\n",
    "    df_2019 = pd.DataFrame(data)\n",
    "    # Set headers if available\n",
    "    if headers:\n",
    "        df_2019.columns = headers\n",
    "        \n",
    "# Making the proper row the column names\n",
    "df_2019.columns = df_2019.iloc[1]\n",
    "df_2019 = df_2019.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Getting rid of rows that have the column names\n",
    "first_two_columns = df_2019.columns[:2]\n",
    "df_2019 = df_2019[~(df_2019[first_two_columns].astype(str) == df_2019.iloc[:, :2].columns).all(axis=1)]\n",
    "\n",
    "# Adding a season column\n",
    "df_2019['Season'] = '18-19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the 5 years of data\n",
    "df_bart = pd.concat([df_2023, df_2022, df_2021, df_2020, df_2019], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the team names\n",
    "df_bart['Team'] = df_bart['Team'].apply(lambda x: re.sub(r'\\d.*', '', x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exctracting total wins from the record column\n",
    "df_bart['Rec'] = df_bart['Rec'].apply(lambda x: re.search(r'^\\d+(?=-)', x).group() if re.search(r'^\\d+(?=-)', x) else x)\n",
    "df_bart.rename(columns={'Rec': 'Wins'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a losses column\n",
    "df_bart['G'] = pd.to_numeric(df_bart['G'])\n",
    "df_bart['Wins'] = pd.to_numeric(df_bart['Wins'])\n",
    "df_bart['Losses'] = df_bart['G'] - df_bart['Wins']\n",
    "columns = list(df_bart.columns)\n",
    "columns.insert(5, columns.pop(columns.index('Losses')))\n",
    "df_bart = df_bart[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the WAB column\n",
    "df_bart.drop(columns=['WAB'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the Barthag column\n",
    "df_bart['Barthag'] = df_bart['Barthag'].apply(lambda x: re.search(r'\\.\\d{4}', str(x)).group(0) if re.search(r'\\.\\d{4}', str(x)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the AdjOE and AdjDE columns\n",
    "def keep_one_decimal(value):\n",
    "    return re.sub(r'(\\.\\d).*', r'\\1', str(value))\n",
    "df_bart['AdjOE'] = df_bart['AdjOE'].apply(keep_one_decimal)\n",
    "df_bart['AdjDE'] = df_bart['AdjDE'].apply(keep_one_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to convert to float\n",
    "columns_to_convert = [\n",
    "    'AdjOE', 'AdjDE', 'Barthag', 'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB',\n",
    "    'FTR', 'FTRD', '2P%', '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.'\n",
    "]\n",
    "\n",
    "df_bart[columns_to_convert] = df_bart[columns_to_convert].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>1</th>\n",
       "      <th>Rk</th>\n",
       "      <th>Team</th>\n",
       "      <th>Conf</th>\n",
       "      <th>G</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Losses</th>\n",
       "      <th>AdjOE</th>\n",
       "      <th>AdjDE</th>\n",
       "      <th>Barthag</th>\n",
       "      <th>EFG%</th>\n",
       "      <th>...</th>\n",
       "      <th>FTR</th>\n",
       "      <th>FTRD</th>\n",
       "      <th>2P%</th>\n",
       "      <th>2P%D</th>\n",
       "      <th>3P%</th>\n",
       "      <th>3P%D</th>\n",
       "      <th>3PR</th>\n",
       "      <th>3PRD</th>\n",
       "      <th>Adj T.</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>BE</td>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>121.5</td>\n",
       "      <td>91.2</td>\n",
       "      <td>.9643</td>\n",
       "      <td>53.933</td>\n",
       "      <td>...</td>\n",
       "      <td>30.8199</td>\n",
       "      <td>37.8312</td>\n",
       "      <td>53.650</td>\n",
       "      <td>44.49</td>\n",
       "      <td>36.359</td>\n",
       "      <td>29.712</td>\n",
       "      <td>41.766</td>\n",
       "      <td>30.413</td>\n",
       "      <td>66.7200</td>\n",
       "      <td>22-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>SEC</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>116.1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>.9548</td>\n",
       "      <td>52.186</td>\n",
       "      <td>...</td>\n",
       "      <td>36.647</td>\n",
       "      <td>32.6220</td>\n",
       "      <td>53.840</td>\n",
       "      <td>40.81</td>\n",
       "      <td>33.5193</td>\n",
       "      <td>28.33</td>\n",
       "      <td>47.29</td>\n",
       "      <td>3010</td>\n",
       "      <td>72.64</td>\n",
       "      <td>22-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Amer</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>117.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>.9532</td>\n",
       "      <td>52.379</td>\n",
       "      <td>...</td>\n",
       "      <td>28.9262</td>\n",
       "      <td>35.7285</td>\n",
       "      <td>53.160</td>\n",
       "      <td>43.45</td>\n",
       "      <td>34173</td>\n",
       "      <td>27.92</td>\n",
       "      <td>37.9160</td>\n",
       "      <td>43.5339</td>\n",
       "      <td>63.4341</td>\n",
       "      <td>22-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>UCLA</td>\n",
       "      <td>P12</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>114.9</td>\n",
       "      <td>88.5</td>\n",
       "      <td>.9528</td>\n",
       "      <td>51.1146</td>\n",
       "      <td>...</td>\n",
       "      <td>27.4305</td>\n",
       "      <td>27.572</td>\n",
       "      <td>50.6161</td>\n",
       "      <td>46.944</td>\n",
       "      <td>34.9123</td>\n",
       "      <td>31.132</td>\n",
       "      <td>29342</td>\n",
       "      <td>38.8234</td>\n",
       "      <td>66.1234</td>\n",
       "      <td>22-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>SEC</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>111.2</td>\n",
       "      <td>87.5</td>\n",
       "      <td>.9401</td>\n",
       "      <td>49.9210</td>\n",
       "      <td>...</td>\n",
       "      <td>30.7203</td>\n",
       "      <td>33.8245</td>\n",
       "      <td>50.3181</td>\n",
       "      <td>45.115</td>\n",
       "      <td>32.8234</td>\n",
       "      <td>26.51</td>\n",
       "      <td>40.1103</td>\n",
       "      <td>41.8317</td>\n",
       "      <td>65.1285</td>\n",
       "      <td>22-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>349</td>\n",
       "      <td>Alcorn St.</td>\n",
       "      <td>SWAC</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>90.2</td>\n",
       "      <td>113.4</td>\n",
       "      <td>.0669</td>\n",
       "      <td>45.7338</td>\n",
       "      <td>...</td>\n",
       "      <td>30.5248</td>\n",
       "      <td>36.5266</td>\n",
       "      <td>45333</td>\n",
       "      <td>55.3336</td>\n",
       "      <td>31.3307</td>\n",
       "      <td>32.154</td>\n",
       "      <td>36.2240</td>\n",
       "      <td>37120</td>\n",
       "      <td>65302</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>350</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>AE</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>84.8</td>\n",
       "      <td>107.0</td>\n",
       "      <td>.0649</td>\n",
       "      <td>44347</td>\n",
       "      <td>...</td>\n",
       "      <td>21.9352</td>\n",
       "      <td>38290</td>\n",
       "      <td>39.4351</td>\n",
       "      <td>52.1260</td>\n",
       "      <td>32.6255</td>\n",
       "      <td>33.6122</td>\n",
       "      <td>48.110</td>\n",
       "      <td>38.4173</td>\n",
       "      <td>65.7267</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>351</td>\n",
       "      <td>Chicago St.</td>\n",
       "      <td>WAC</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>89.6</td>\n",
       "      <td>118.3</td>\n",
       "      <td>.0393</td>\n",
       "      <td>44.2346</td>\n",
       "      <td>...</td>\n",
       "      <td>33.1182</td>\n",
       "      <td>33.9210</td>\n",
       "      <td>43.5347</td>\n",
       "      <td>57.9352</td>\n",
       "      <td>30.7330</td>\n",
       "      <td>38.5337</td>\n",
       "      <td>27352</td>\n",
       "      <td>43.8320</td>\n",
       "      <td>7138</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>352</td>\n",
       "      <td>Delaware St.</td>\n",
       "      <td>MEAC</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>85.4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>.0384</td>\n",
       "      <td>40353</td>\n",
       "      <td>...</td>\n",
       "      <td>25.5327</td>\n",
       "      <td>39.2312</td>\n",
       "      <td>37.7353</td>\n",
       "      <td>52.6271</td>\n",
       "      <td>29347</td>\n",
       "      <td>34.7193</td>\n",
       "      <td>40139</td>\n",
       "      <td>39.5214</td>\n",
       "      <td>70.453</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>353</td>\n",
       "      <td>Maryland Eastern Shore</td>\n",
       "      <td>MEAC</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>86.8</td>\n",
       "      <td>115.2</td>\n",
       "      <td>.0371</td>\n",
       "      <td>43.5350</td>\n",
       "      <td>...</td>\n",
       "      <td>28.3293</td>\n",
       "      <td>36.6269</td>\n",
       "      <td>44.5340</td>\n",
       "      <td>53.2287</td>\n",
       "      <td>27.9352</td>\n",
       "      <td>37.3311</td>\n",
       "      <td>36.2240</td>\n",
       "      <td>44324</td>\n",
       "      <td>63.7333</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1774 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "1      Rk                    Team  Conf   G  Wins  Losses  AdjOE  AdjDE  \\\n",
       "0       1             Connecticut    BE  39    31       8  121.5   91.2   \n",
       "1       2                 Alabama   SEC  37    31       6  116.1   89.0   \n",
       "2       3                 Houston  Amer  37    33       4  117.3   90.3   \n",
       "3       4                    UCLA   P12  37    31       6  114.9   88.5   \n",
       "4       5               Tennessee   SEC  36    25      11  111.2   87.5   \n",
       "...   ...                     ...   ...  ..   ...     ...    ...    ...   \n",
       "1769  349              Alcorn St.  SWAC  27    10      17   90.2  113.4   \n",
       "1770  350           New Hampshire    AE  27     5      22   84.8  107.0   \n",
       "1771  351             Chicago St.   WAC  30     3      27   89.6  118.3   \n",
       "1772  352            Delaware St.  MEAC  29     6      23   85.4  113.0   \n",
       "1773  353  Maryland Eastern Shore  MEAC  30     7      23   86.8  115.2   \n",
       "\n",
       "1    Barthag     EFG%  ...      FTR     FTRD      2P%     2P%D      3P%  \\\n",
       "0      .9643   53.933  ...  30.8199  37.8312   53.650    44.49   36.359   \n",
       "1      .9548   52.186  ...   36.647  32.6220   53.840    40.81  33.5193   \n",
       "2      .9532   52.379  ...  28.9262  35.7285   53.160    43.45    34173   \n",
       "3      .9528  51.1146  ...  27.4305   27.572  50.6161   46.944  34.9123   \n",
       "4      .9401  49.9210  ...  30.7203  33.8245  50.3181   45.115  32.8234   \n",
       "...      ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "1769   .0669  45.7338  ...  30.5248  36.5266    45333  55.3336  31.3307   \n",
       "1770   .0649    44347  ...  21.9352    38290  39.4351  52.1260  32.6255   \n",
       "1771   .0393  44.2346  ...  33.1182  33.9210  43.5347  57.9352  30.7330   \n",
       "1772   .0384    40353  ...  25.5327  39.2312  37.7353  52.6271    29347   \n",
       "1773   .0371  43.5350  ...  28.3293  36.6269  44.5340  53.2287  27.9352   \n",
       "\n",
       "1        3P%D      3PR     3PRD   Adj T. Season  \n",
       "0      29.712   41.766   30.413  66.7200  22-23  \n",
       "1       28.33    47.29     3010    72.64  22-23  \n",
       "2       27.92  37.9160  43.5339  63.4341  22-23  \n",
       "3      31.132    29342  38.8234  66.1234  22-23  \n",
       "4       26.51  40.1103  41.8317  65.1285  22-23  \n",
       "...       ...      ...      ...      ...    ...  \n",
       "1769   32.154  36.2240    37120    65302  18-19  \n",
       "1770  33.6122   48.110  38.4173  65.7267  18-19  \n",
       "1771  38.5337    27352  43.8320     7138  18-19  \n",
       "1772  34.7193    40139  39.5214   70.453  18-19  \n",
       "1773  37.3311  36.2240    44324  63.7333  18-19  \n",
       "\n",
       "[1774 rows x 25 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_bart.to_csv('CBBdata19-23.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still work in progress\n",
    "\n",
    "# Scraping data from ESPN\n",
    "#url = 'https://www.espn.com/mens-college-basketball/stats/team/_/view/differential/season/2023'\n",
    "#headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "#response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GP    PTS   FGM   FGA   FG%   3PM    3PA   3P%   FTM   FTA   FT%    OR  \\\n",
      "0  37  +17.4  +8.1  +7.2  +9.2  +1.3   -0.1  +6.1  -0.2  -1.4  +4.3  +2.8   \n",
      "1  39  +14.4  +5.7  +4.8  +6.4  +4.0   +8.2  +6.5  -1.2  -2.3  +2.6  +3.7   \n",
      "2  36  +13.7  +4.7  +3.9  +5.5  +4.0   +8.8  +3.9  +0.1  +0.3  -0.6  +1.5   \n",
      "3  37  +13.5  +3.7  -1.9  +7.1  +4.4  +10.2  +5.2  +1.6  +1.8  +1.4  +0.8   \n",
      "4  37  +13.4  +5.9  +6.8  +5.2  -0.3   -3.2  +3.7  +1.8  +1.8  +3.3  +1.8   \n",
      "\n",
      "     DR   REB   AST   STL   BLK    TO    PF  \n",
      "0  +4.4  +7.2  +4.5  +2.5  +2.4  -3.6  +0.5  \n",
      "1  +5.5  +9.2  +8.3  -0.3  +2.1  +0.2  +0.7  \n",
      "2  +2.0  +3.5  +7.0  +0.5  -0.2  -2.7  -0.8  \n",
      "3  +6.4  +7.2  +5.1  -1.1  +1.2  +2.1  -1.1  \n",
      "4  +1.1  +3.0  +3.3  +3.7  +1.1  -5.6  -1.0  \n"
     ]
    }
   ],
   "source": [
    "# if response.status_code == 200:\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     table = soup.find('table', class_='Table Table--align-right')\n",
    "\n",
    "#     if table:\n",
    "#         # Extract table data\n",
    "#         rows = table.find_all('tr')\n",
    "#         data = []\n",
    "#         for row in rows:\n",
    "#             cols = row.find_all(['td', 'th'])\n",
    "#             cols = [col.text.strip() for col in cols]\n",
    "#             data.append(cols)\n",
    "\n",
    "#         # Convert data to DataFrame\n",
    "#         df = pd.DataFrame(data[1:], columns=data[0])\n",
    "#         print(df.head())  # Display the first few rows of the DataFrame\n",
    "#     else:\n",
    "#         print('Table not found on the page.')\n",
    "# else:\n",
    "#     print('Failed to fetch the webpage.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat386",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
